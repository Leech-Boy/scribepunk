---
layout: default
title: "Artificial Intelligence"
categories: substack
---

<h1>AI</h1>
<!--excerpt-start-->
<p>Artificial Intelligence is a topic that is constantly in the news, and for good reason. AI programs have done things that may have once seemed impossible for a computer to do. They can respond to written language in a way that seems human, they can create images from a written prompt, and many more things.</p>
<!--excerpt-end-->

<p>As a computer science student, I am often asked my opinions on AI. I agree that it is remarkable, but I don’t think it’s all that it’s cracked up to be. As I will describe, there are severe limitations in its abilities that cannot be avoided without significant changes to how AI programs are created.</p>

<h1>Weaknesses of AI</h1>
<p>There are certain inherent differences in human and machine minds. An AI program may be trained and/or executed on an arbitrarily powerful computer and so the amount of data that an AI can input, output, and process is only limited by the computer that it runs on. However, our human minds can only be “executed” within our very limited bodies. This is a fundamental difference between human and AI minds.</p>

<p>However, weakness is our intellectual superpower. Because of this weakness we are forced to create clever strategies for solving problems that an AI is not forced to do. We do not have the time or energy to apply a “one size fits all” approach to every problem, so we learn to apply intuition to the problems that can be solved quickest with intuition, and we learn to apply reason to the problems that can be solved quickest by reason. We achieve efficiency, because we must. AI does not have this necessity, so it is inefficient.</p>

<p>Problem solving strategies fall on a continuum from intuition to conscious reasoning. A problem like identifying an animal as a cat or dog is something that is done intuitively. As soon as you see one of those animals you immediately identify it. We do not distinguish these animals with deliberate thinking. On the other hand, a task like choosing a meal off of a menu is a task that uses deliberate, conscious reasoning.</p>

<p>Currently “Artificial Intelligence” refers to programs which are trained to solve problems through trial and error. This is the same way that people learn intuitive skills. This does not mean that problems like choosing a meal off of a menu are impossible for AI to solve. It simply means that an AI will solve it using intuition, unlike a human.</p>

<p>This is by design. Each AI is designed to perform a specific task. An AI, when given a problem will focus completely on solving the problem. Humans on the other hand don’t ever devote their entire brain to the solution of a problem. Even if you focus as hard as you can, you still process thoughts on a subconscious level about the sights, sounds, and smells around you, not to mention the conscious thoughts that cross your mind uninvited, and more interestingly, even after solving a problem you may think of it from time to time.</p>

<p>It is these thoughts that continue past the solving of a problem that allow us to apply the solution of one problem to another problem. This abstract reasoning is something that AI is unable to do, because it is unable to have extraneous thoughts or memories.</p>

<p>These thoughts are a kind of mental tool. They are mental objects that we create using our higher mental processes. They often take the form of words, images, or some other kind of symbol. The words “cat” and “doghouse” are symbols. When we use either of these words, we do not need to refer to specific cats or doghouses, we can refer to the idea itself, and we can treat the ideas the same way we treat real objects. You can take the idea of a cat and the idea of a doghouse and combine them in your mind to create the idea of a cat in a doghouse, without ever having seen a cat in a doghouse, in the same way that you could put an actual cat in an actual doghouse.</p>

<h1>Obstacles</h1>
<p>Suppose an AI was built so that it could create these mental objects. How could we communicate about them? Currently all communication between humans and AI follows rules laid out by the programmers. During training, an input is sent to the AI, the AI responds by sending an output, and the AI is either told that it was correct or incorrect. Communication with an AI once it is done training is the same except that there is no communication about whether the output was correct.</p>

<p>If an AI wanted to communicate that it had created a mental object, how would it communicate this? Everything that it outputs will be interpreted as a solution to the problem it is given. Perhaps the AI could be configured so that the solution to the problem is expected to be a mental object. The AI would necessarily fail in creating mental objects the way that humans do. It would create them when asked, and never use them for problem solving when the whole point of them is to be applied. So, the communication of the object must be along a separate channel of communication than the solution to the problem itself.</p>

<p>For example, a text generation program may respond to the prompt “tell me a joke”, with “3 men walked into a bar, their heads were severely injured”, and along a separate avenue communicate “concept 31”. Through trial and error, the programmers may learn that the mental object “concept 31” refers to the idea of humor, or perhaps a more specific concept like dry humor.</p>

<p>This is exactly how humans create simple mental objects. When we are young a parent or other person may point to a cat or dog and say “cat” or “dog” while pointing. This demonstrates the two avenues of communication. There is the speaking and the pointing. The speaking communicates what the mental object is, either the word “cat” or the word “dog”. The pointing demonstrates one instance of a thing that the mental object can refer to.</p>

<h1>AGI</h1>
<p>There are those who believe that with powerful enough computers, and large enough data sets, we will be able to create AGI, Artificial General Intelligence. I do not think that this is true. AGI is an artificial intelligence that is generalized, in other words it could learn a skill, like mastering poker, and generalize that skill by applying some of the same tactics to business negotiation.</p>

<p>I believe that in order to create ideas that can transfer across domains, you must understand what the ideas mean. I don’t think that a program could transfer the skills learned in poker to business negotiation without understanding what poker and business negotiation are, well enough to see their similarities.</p>

<p>If an AI is trained only on text data, I do not think that it would ever have true understanding of its ideas. Given only one avenue of communication, it is impossible to understand what the communication refers to. The example of an adult pointing to a dog or cat and saying the associated word for a child to learn illustrates this point.</p>

<h1>Conclusion</h1>
<p>I think that AI is a remarkable tool, but it is not intelligent in the way that humans are intelligent, and I do not believe that it could become truly intelligent as long as it is purely a program on a computer. It must have sensors that detect the real world, so that it can understand meaning, or it must be given a simulation to play in. This is not to say that AI is useless, or harmless. An AI could conceivably wipe out humanity without even understanding what it was doing, see the paperclip maximizer thought experiment.</p>
